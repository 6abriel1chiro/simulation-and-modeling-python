# -*- coding: utf-8 -*-
"""clase3009 - 0310 gradient descent multivariable .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sBspJ2gmEE1jmXiJavcjTxmnEvafwXVa
"""

import numpy as np
import matplotlib.pyplot as plt
DATAPATH='/content/sample_data/datosMulti.txt'

from typing_extensions import Self
class Regression:
    # cosntructor
    def __init__(self):
        self.X = None
        self.y = None
        self.theta = None
        self.__history = None

    def fit(self, x, y):
        m, n = x.shape
        self.X = np.append(np.ones((m, 1)), x.reshape(m,-1), axis=1)
        self.y = y.reshape(-1, 1)
        self.theta = np.zeros(n + 1)

    def initialize(self):
      m, n = self.X.shape
      self.theta = np.zeros(n)

    
    def normalize(self):
      u = self.X[:,1:].mean(0)
      desv = self.X[:,1:].std(0)
      self.X[:,1:] = (self.X[:,1:] - u) /desv


    def graph(self, modelo=False):
        fig = plt.figure()
        ax = fig.gca(projection='3d')
        plt.scatter(self.X[:, 1], self.X[:, 2], self.y)
        plt.show()
# funcion costo
    def getJ(self, theta):
      theta = theta.reshape(-1,1)
      m = self.X.shape[0]
      h = self.X.dot(theta)
      error = h - self.y 
      j = 1 / (2 *  m) * np.power(error, 2).sum()
      return j

    def getGradient(self, theta):
      theta = theta.reshape(-1, 1)
      m = self.X.shape[0]
      h = self.X.dot(theta)
      error = h - self.y
      t = 1 / m * self.X.T.dot(error)
      return t.flatten()

    def gradientDescent(self, alpha, epsilon=10e-6, iter = None):
        js = []
        theta = self.theta
        i = 0
        while True:
            js.append(self.getJ(theta))
            theta = theta - alpha * self.getGradient(theta) # 1x3 - alpha * 1x3gradient
            if abs(self.getJ(theta) - js[-1]) < epsilon:
                break
            elif iter is not None:
              if i>=iter:
                break
            i = i +1 
        self.theta = theta
        self.__history = np.array(js)


    def getNormalEq(self):
      self.theta = (np.linalg.pinv((self.X.T.dot(self.X))).dot(self.X.T)).dot(self.y)
    
    def graph_history(self):
      plt.plot(range(self.__history.size), self.__history)
      plt.grid()
      plt.xlabel("iteraciones")
      plt.ylabel(r"$J(\theta)$")
      plt.title("Evoluci√≥n de costo en el descenso de Gradente")
      plt.show()


    # solo valido si solo se toma en cuenta dos variables caracteristicas
    def graph2dData(self, model=False):
        fig = plt.figure()
        ax = fig.gca(projection='3d')
        ax.scatter(self.X[:, 1], self.X[:, 2], self.y)
        if model:
            # calculamos los valores del plano para los puntos x e y
            xx1 = np.linspace(self.X[:, 1].min(), self.X[:, 1].max(), 100)
            xx2 = np.linspace(self.X[:, 2].min(), self.X[:, 2].max(), 100)
            xx1, xx2 = np.meshgrid(xx1, xx2)
            x1 = (self.theta[1] * xx1)
            x2 = (self.theta[2] * xx2)
            z = (x1 + x2 + self.theta[0])
            ax.plot_surface(xx1, xx2, z, alpha=0.4, cmap='hot')
        plt.show()

    # solo valido si solo se toma en cuenta una variable caracteristica
    def graph1dData(self, model=False):
        plt.scatter(self.X[:, 1], self.y)
        if model:
            x = np.linspace(self.X[:, 1].min(), self.X[:, 1].max(), 100)
            plt.plot(x, self.theta[0] + self.theta[1] * x, c="red")
        plt.grid()
        plt.show()

    # solo valido si solo se toma en cuenta una variable caracteristica
    def graphJ3d(self):
        fig = plt.figure()
        ax = fig.gca(projection='3d')

        theta0 = np.linspace(-10, 10, 100)
        theta1 = np.linspace(-1, 4, 100)

        theta0, theta1 = np.meshgrid(theta0, theta1)
        j_cost = np.zeros((100, 100))

        for i in range(100):
            for j in range(100):
                t = np.array([theta0[i, j], theta1[i, j]])
                j_cost[i, j] = self.get_j(t)
        ax.scatter(self.theta[0], self.theta[1], self.get_j(self.theta), c='red')

        ax.plot_surface(theta0, theta1, j_cost)
        plt.xlabel("theta0")
        plt.ylabel("theta1")
        plt.show()

    def graphJCurves(self):

        theta0 = np.linspace(-10, 10, 100)
        theta1 = np.linspace(-1, 4, 100)
        theta0, theta1 = np.meshgrid(theta0, theta1)
        j_costo = np.zeros((100, 100))

        for i in range(100):
            for j in range(100):
                t = np.array([theta0[i, j], theta1[i, j]])
                j_costo[i, j] = self.get_j(t)

        plt.contour(theta0, theta1, j_costo, np.logspace(-2, 3, 20))
        plt.scatter(self.theta[0], self.theta[1], c='red')
        plt.xlabel("theta0")
        plt.ylabel("theta1")
        plt.grid()
        plt.show()

data  = np.loadtxt(DATAPATH, delimiter = ",")
data

r = Regression()



x = data[:,:-1]
y = data[:,-1]

r.fit(x,y)

r.normalize()

r.gradientDescent(0.01)

r.graph_history()

